{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "import re\n",
    "from langchain.text_splitter import TextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 문서 로드(Load Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 4272\n"
     ]
    }
   ],
   "source": [
    "class JSONLLoader(BaseLoader):\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load(self):\n",
    "        documents = []\n",
    "        seq_num = 1\n",
    "        \n",
    "        with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                data = json.loads(line)\n",
    "                doc = Document(\n",
    "                    page_content=data['content'],\n",
    "                    metadata={\n",
    "                        'docid': data['docid'],\n",
    "                        'src': data.get('src', ''),  # 'src' 필드가 없을 경우 빈 문자열 사용\n",
    "                        'source': self.file_path,\n",
    "                        'seq_num': seq_num,\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "                seq_num += 1\n",
    "        \n",
    "        return documents\n",
    "\n",
    "# 사용 예시\n",
    "file_path = \"/data/ephemeral/home/upstage-ai-final-ir2/HM/data/documents.jsonl\"\n",
    "loader = JSONLLoader(file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"문서의 수: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 문서 분할(Split Documents)\n",
    "문서를 chunk로 분할(split)하는 것은 RAG(Retrieval-Augmented Generation) 시스템에서 매우 중요한 단계입니다. 이 과정의 주요 이유와 이점은 다음과 같습니다:\n",
    "\n",
    "검색 정확도 향상:\n",
    "\n",
    "큰 문서를 작은 chunk로 나누면 질문과 더 관련성 높은 부분을 정확하게 검색할 수 있습니다.\n",
    "전체 문서보다 특정 chunk가 질문에 더 적합할 가능성이 높습니다.\n",
    "\n",
    "\n",
    "컨텍스트 관리:\n",
    "\n",
    "LLM(Large Language Model)은 입력 토큰 수에 제한이 있습니다. 작은 chunk를 사용하면 이 제한 내에서 더 많은 관련 정보를 포함할 수 있습니다.\n",
    "\n",
    "\n",
    "검색 효율성:\n",
    "\n",
    "벡터 데이터베이스에서 작은 chunk를 검색하는 것이 큰 문서 전체를 검색하는 것보다 빠르고 효율적입니다.\n",
    "\n",
    "\n",
    "메모리 효율성:\n",
    "\n",
    "작은 chunk는 메모리 사용을 줄이고, 대규모 문서 컬렉션을 효율적으로 처리할 수 있게 합니다.\n",
    "\n",
    "\n",
    "세분화된 정보 접근:\n",
    "\n",
    "문서의 특정 부분에 집중할 수 있어, 더 정확하고 관련성 높은 응답을 생성할 수 있습니다.\n",
    "\n",
    "\n",
    "중복 제거와 정보 필터링:\n",
    "\n",
    "chunk 생성 과정에서 중복된 정보를 제거하거나 불필요한 부분을 필터링할 수 있습니다.\n",
    "\n",
    "\n",
    "다양한 소스 통합:\n",
    "\n",
    "여러 문서의 chunk를 효과적으로 조합하여 종합적인 정보를 제공할 수 있습니다.\n",
    "\n",
    "\n",
    "컨텍스트 윈도우 최적화:\n",
    "\n",
    "chunk 크기를 조절하여 LLM에 제공되는 컨텍스트의 양을 최적화할 수 있습니다.\n",
    "\n",
    "\n",
    "임베딩 품질 향상:\n",
    "\n",
    "작은 chunk는 더 집중된 의미를 가질 수 있어, 더 정확한 임베딩을 생성할 수 있습니다.\n",
    "\n",
    "\n",
    "동적 컨텍스트 구성:\n",
    "\n",
    "질문에 따라 가장 관련성 높은 chunk들을 동적으로 선택하여 컨텍스트를 구성할 수 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "문서를 chunk로 분할하는 것은 RAG 시스템의 성능, 효율성, 정확성을 크게 향상시킬 수 있는 중요한 전처리 단계입니다. 하지만 적절한 chunk 크기와 분할 방법을 선택하는 것이 중요하며, 이는 문서의 특성과 사용 사례에 따라 달라질 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 4272\n",
      "분할된 문서의 수: 4280\n"
     ]
    }
   ],
   "source": [
    "# RecursiveCharacterTextSplitter \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# 문서 분할\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"문서의 수: {len(documents)}\")\n",
    "print(f\"분할된 문서의 수: {len(split_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 4272\n",
      "분할된 문서의 수: 27964\n"
     ]
    }
   ],
   "source": [
    "# 문장 단위 TextSplitter\n",
    "class SentenceSplitter(TextSplitter):\n",
    "    def split_text(self, text: str) -> list[str]:\n",
    "        # 문장 끝 패턴: .!?로 끝나고 공백이 따라오는 경우\n",
    "        # 줄바꿈 문자도 문장의 끝으로 간주\n",
    "        return [sentence.strip() for sentence in re.split(r'(?<=[.!?])\\s+|\\n', text) if sentence.strip()]\n",
    "\n",
    "# SentenceSplitter 초기화\n",
    "sentence_splitter = SentenceSplitter()\n",
    "\n",
    "# 문서 분할\n",
    "split_documents = sentence_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"문서의 수: {len(documents)}\")\n",
    "print(f\"분할된 문서의 수: {len(split_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envforir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
