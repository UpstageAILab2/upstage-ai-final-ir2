{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/home/envforir/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith Project: EXP04_GC\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "UPSTAGE_API_KEY = os.environ.get('UPSTAGE_API_KEY')\n",
    "LANGCHAIN_API_KEY = os.environ.get('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'EXP04_GC' # 프로젝트명 수정\n",
    "LANGCHAIN_PROJECT = os.environ.get('LANGCHAIN_PROJECT')\n",
    "\n",
    "print(f'LangSmith Project: {LANGCHAIN_PROJECT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 구성\n",
    "file_path = '../data/documents.jsonl'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=file_path,\n",
    "    jq_schema='.',\n",
    "    text_content=False,\n",
    "    json_lines=True,\n",
    ")\n",
    "temp = loader.load()\n",
    "\n",
    "seq_num = 1\n",
    "documents = []\n",
    "for tmp in temp:\n",
    "    data = json.loads(tmp.page_content)\n",
    "    doc = Document(page_content=data['content'], metadata={\n",
    "        'docid': data['docid'],\n",
    "        'src': data['src'],\n",
    "        'source': '/data/ephemeral/home/upstage-ai-final-ir2/upstage-ai-final-ir2/HM/data/documents.jsonl',\n",
    "        'seq_num': seq_num,\n",
    "    })\n",
    "    documents.append(doc)\n",
    "    seq_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator='',\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "split_documents = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "embeddings = UpstageEmbeddings(\n",
    "    api_key=UPSTAGE_API_KEY, \n",
    "    model=\"solar-embedding-1-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store 생성 중\n",
      "Vector Store 생성 및 로컬 저장 완료\n"
     ]
    }
   ],
   "source": [
    "# 벡터 저장소 생성\n",
    "# pip install faiss-cpu\n",
    "folder_path = f'./faiss_{LANGCHAIN_PROJECT}'\n",
    "if not os.path.exists(folder_path):\n",
    "    print('Vector Store 생성 중')\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=split_documents,\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "    vectorstore.save_local(folder_path=folder_path)\n",
    "    print('Vector Store 생성 및 로컬 저장 완료')\n",
    "else:\n",
    "    vectorstore = FAISS.load_local(\n",
    "        folder_path=folder_path, \n",
    "        embeddings=embeddings, \n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    print('Vector Store 로컬에서 불러옴')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 구현에 필요한 Question Answering을 위한 LLM  프롬프트\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM과 검색엔진을 활용한 RAG 구현 (기존 코드와 동일)\n",
    "retriever = vectorstore.as_retriever(k=5)\n",
    "chat = ChatUpstage(model='solar-1-mini-chat', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groundedness Check 함수 추가\n",
    "def check_groundedness(context, response):\n",
    "    url = \"https://api.upstage.ai/v1/solar/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {UPSTAGE_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # data = {\n",
    "    #     \"model\": \"solar-1-mini-groundedness-check\",\n",
    "    #     \"messages\": [\n",
    "    #         {\"role\": \"user\", \"content\": context},\n",
    "    #         {\"role\": \"assistant\", \"content\": response}\n",
    "    #     ],\n",
    "    #     \"temperature\": 0\n",
    "    # }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"solar-1-mini-groundedness-check\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": messages[0][\"content\"]},\n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ],\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "    else:\n",
    "        return \"Error in groundedness check\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    global references\n",
    "    references = docs\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(messages):\n",
    "    global references\n",
    "    response = {\"topk\": \"\", \"answer\": \"\", \"references\": \"\", \"groundedness\": \"\"}\n",
    "\n",
    "    rag_chain = (\n",
    "        {'context': retriever | format_docs, 'question': RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | chat\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    history = '\\n'.join([f\"{message['role']}: {message['content']}\" for message in messages]) + '\\n'\n",
    "    answer = rag_chain.invoke(history)\n",
    "\n",
    "    ref_content = [reference.page_content for reference in references]\n",
    "    topk = [reference.metadata['docid'] for reference in references]\n",
    "\n",
    "    # Groundedness Check 수행\n",
    "    context = '\\n'.join(ref_content)\n",
    "    groundedness = check_groundedness(context, answer)\n",
    "\n",
    "    response[\"topk\"] = topk\n",
    "    response[\"answer\"] = answer\n",
    "    response[\"references\"] = ref_content\n",
    "    response[\"groundedness\"] = groundedness\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_non_grounded_results(output_filename):\n",
    "    non_grounded = []\n",
    "    with open(output_filename, 'r') as file:\n",
    "        for line in file:\n",
    "            result = json.loads(line)\n",
    "            if result['groundedness'] != 'grounded':\n",
    "                non_grounded.append(result)\n",
    "    \n",
    "    if non_grounded:\n",
    "        print(f\"\\n총 {len(non_grounded)}개의 non-grounded 결과가 있습니다:\\n\")\n",
    "        for item in non_grounded:\n",
    "            print(f\"Eval ID: {item['eval_id']}\")\n",
    "            print(f\"Question: {item.get('question', 'N/A')}\")  # 질문이 저장되어 있다면 출력\n",
    "            print(f\"Answer: {item['answer']}\")\n",
    "            print(f\"References:\")\n",
    "            for ref in item['references']:\n",
    "                print(f\"  - {ref}\")\n",
    "            print(f\"Groundedness: {item['groundedness']}\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"\\n모든 결과가 grounded입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가를 위한 파일을 읽어서 각 평가 데이터에 대해서 결과 추출후 파일에 저장\n",
    "def eval_rag(eval_filename, output_filename):\n",
    "    with open(eval_filename) as eval_lines, open(output_filename, 'w') as output_lines:\n",
    "        idx = 0\n",
    "        for eval_line in eval_lines:\n",
    "            j = json.loads(eval_line)\n",
    "            print(f'Test {idx}\\nQuestion: {j[\"msg\"]}')\n",
    "            response = answer_question(j[\"msg\"])\n",
    "            print(f'Answer: {response[\"answer\"]}')\n",
    "            print(f'Groundedness: {response[\"groundedness\"]}\\n')\n",
    "\n",
    "            output = {\n",
    "                \"eval_id\": j[\"eval_id\"], \n",
    "                \"question\": j[\"msg\"][-1][\"content\"],  # 질문 저장\n",
    "                \"topk\": response[\"topk\"], \n",
    "                \"answer\": response[\"answer\"], \n",
    "                \"references\": response[\"references\"],\n",
    "                \"groundedness\": response[\"groundedness\"]\n",
    "            }\n",
    "            output_lines.write(f'{json.dumps(output, ensure_ascii=False)}\\n')\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0\n",
      "Question: [{'role': 'user', 'content': '나무의 분류에 대해 조사해 보기 위한 방법은?'}]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'messages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 평가 실행\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43meval_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/eval.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../output/EXP04_GC.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[43], line 8\u001b[0m, in \u001b[0;36meval_rag\u001b[0;34m(eval_filename, output_filename)\u001b[0m\n\u001b[1;32m      6\u001b[0m j \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(eval_line)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43manswer_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mj\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmsg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGroundedness: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroundedness\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[41], line 20\u001b[0m, in \u001b[0;36manswer_question\u001b[0;34m(messages)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Groundedness Check 수행\u001b[39;00m\n\u001b[1;32m     19\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ref_content)\n\u001b[0;32m---> 20\u001b[0m groundedness \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_groundedness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopk\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m topk\n\u001b[1;32m     23\u001b[0m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m answer\n",
      "Cell \u001b[0;32mIn[39], line 21\u001b[0m, in \u001b[0;36mcheck_groundedness\u001b[0;34m(context, response)\u001b[0m\n\u001b[1;32m      4\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mUPSTAGE_API_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# data = {\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#     \"model\": \"solar-1-mini-groundedness-check\",\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     \"messages\": [\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     \"temperature\": 0\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[1;32m     18\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolar-1-mini-groundedness-check\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m---> 21\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmessages\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]},\n\u001b[1;32m     22\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: answer}\n\u001b[1;32m     23\u001b[0m     ],\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     25\u001b[0m }\n\u001b[1;32m     27\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'messages' is not defined"
     ]
    }
   ],
   "source": [
    "# 평가 실행\n",
    "eval_rag('../data/eval.jsonl', '../output/EXP04_GC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 결과 중 non-grounded 항목 출력\n",
    "print_non_grounded_results('../output/EXP04_GC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envforir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
