{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "import re\n",
    "from langchain.text_splitter import TextSplitter\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import re\n",
    "# from langchain.text_splitter import SemanticChunker\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pickle\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 문서 로드(Load Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 4272\n"
     ]
    }
   ],
   "source": [
    "class JSONLLoader(BaseLoader):\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load(self):\n",
    "        documents = []\n",
    "        seq_num = 1\n",
    "        \n",
    "        with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                data = json.loads(line)\n",
    "                doc = Document(\n",
    "                    page_content=data['content'],\n",
    "                    metadata={\n",
    "                        'docid': data['docid'],\n",
    "                        'src': data.get('src', ''),  # 'src' 필드가 없을 경우 빈 문자열 사용\n",
    "                        'source': self.file_path,\n",
    "                        'seq_num': seq_num,\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "                seq_num += 1\n",
    "        \n",
    "        return documents\n",
    "\n",
    "# 사용 예시\n",
    "file_path = \"/data/ephemeral/home/upstage-ai-final-ir2/HM/data/documents.jsonl\"\n",
    "loader = JSONLLoader(file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"문서의 수: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 문서 분할(Split Documents)\n",
    "문서를 chunk로 분할(split)하는 것은 RAG(Retrieval-Augmented Generation) 시스템에서 매우 중요한 단계입니다. 이 과정의 주요 이유와 이점은 다음과 같습니다:\n",
    "\n",
    "검색 정확도 향상:\n",
    "\n",
    "큰 문서를 작은 chunk로 나누면 질문과 더 관련성 높은 부분을 정확하게 검색할 수 있습니다.\n",
    "전체 문서보다 특정 chunk가 질문에 더 적합할 가능성이 높습니다.\n",
    "\n",
    "\n",
    "컨텍스트 관리:\n",
    "\n",
    "LLM(Large Language Model)은 입력 토큰 수에 제한이 있습니다. 작은 chunk를 사용하면 이 제한 내에서 더 많은 관련 정보를 포함할 수 있습니다.\n",
    "\n",
    "\n",
    "검색 효율성:\n",
    "\n",
    "벡터 데이터베이스에서 작은 chunk를 검색하는 것이 큰 문서 전체를 검색하는 것보다 빠르고 효율적입니다.\n",
    "\n",
    "\n",
    "메모리 효율성:\n",
    "\n",
    "작은 chunk는 메모리 사용을 줄이고, 대규모 문서 컬렉션을 효율적으로 처리할 수 있게 합니다.\n",
    "\n",
    "\n",
    "세분화된 정보 접근:\n",
    "\n",
    "문서의 특정 부분에 집중할 수 있어, 더 정확하고 관련성 높은 응답을 생성할 수 있습니다.\n",
    "\n",
    "\n",
    "중복 제거와 정보 필터링:\n",
    "\n",
    "chunk 생성 과정에서 중복된 정보를 제거하거나 불필요한 부분을 필터링할 수 있습니다.\n",
    "\n",
    "\n",
    "다양한 소스 통합:\n",
    "\n",
    "여러 문서의 chunk를 효과적으로 조합하여 종합적인 정보를 제공할 수 있습니다.\n",
    "\n",
    "\n",
    "컨텍스트 윈도우 최적화:\n",
    "\n",
    "chunk 크기를 조절하여 LLM에 제공되는 컨텍스트의 양을 최적화할 수 있습니다.\n",
    "\n",
    "\n",
    "임베딩 품질 향상:\n",
    "\n",
    "작은 chunk는 더 집중된 의미를 가질 수 있어, 더 정확한 임베딩을 생성할 수 있습니다.\n",
    "\n",
    "\n",
    "동적 컨텍스트 구성:\n",
    "\n",
    "질문에 따라 가장 관련성 높은 chunk들을 동적으로 선택하여 컨텍스트를 구성할 수 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "문서를 chunk로 분할하는 것은 RAG 시스템의 성능, 효율성, 정확성을 크게 향상시킬 수 있는 중요한 전처리 단계입니다. 하지만 적절한 chunk 크기와 분할 방법을 선택하는 것이 중요하며, 이는 문서의 특성과 사용 사례에 따라 달라질 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 4272\n",
      "분할된 문서의 수: 4280\n"
     ]
    }
   ],
   "source": [
    "# RecursiveCharacterTextSplitter \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# 문서 분할\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"문서의 수: {len(documents)}\")\n",
    "print(f\"분할된 문서의 수: {len(split_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 문서의 수: 4272\n",
      "분할된 문서의 수: 27964\n"
     ]
    }
   ],
   "source": [
    "# 문장 단위 TextSplitter\n",
    "class SentenceSplitter(TextSplitter):\n",
    "    def split_text(self, text: str) -> list[str]:\n",
    "        # 문장 끝 패턴: .!?로 끝나고 공백이 따라오는 경우\n",
    "        # 줄바꿈 문자도 문장의 끝으로 간주\n",
    "        return [sentence.strip() for sentence in re.split(r'(?<=[.!?])\\s+|\\n', text) if sentence.strip()]\n",
    "\n",
    "# SentenceSplitter 초기화\n",
    "sentence_splitter = SentenceSplitter()\n",
    "\n",
    "# 문서 분할\n",
    "sentence_split_documents = sentence_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"원본 문서의 수: {len(documents)}\")\n",
    "print(f\"분할된 문서의 수: {len(split_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 문서의 수: 4272\n",
      "분할된 문서의 수: 27964\n"
     ]
    }
   ],
   "source": [
    "# SemanticChunker\n",
    "semantic_text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(), add_start_index=True)\n",
    "\n",
    "# documents를 split\n",
    "semantic_split_documents = semantic_text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"원본 문서의 수: {len(documents)}\")\n",
    "print(f\"분할된 문서의 수: {len(semantic_split_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 문서의 수: 4272\n",
      "분할된 문서의 수: 8540\n"
     ]
    }
   ],
   "source": [
    "print(f\"원본 문서의 수: {len(documents)}\")\n",
    "print(f\"분할된 문서의 수: {len(semantic_split_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_documents 저장\n",
    "with open('semantic_split_documents.pkl', 'wb') as f:\n",
    "    pickle.dump(semantic_split_documents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로드된 분할 문서의 수: 8540\n"
     ]
    }
   ],
   "source": [
    "# split_documents 로드\n",
    "with open('semantic_split_documents.pkl', 'rb') as f:\n",
    "    loaded_split_documents = pickle.load(f)\n",
    "\n",
    "print(f\"로드된 분할 문서의 수: {len(loaded_split_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 임베딩(Embedding) 및 벡터저장소 생성(Create Vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/root/home/envforir/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:55\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[0;34m(no_avx2)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# semantic_split_documents로 벡터저장소 생성 \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msemantic_split_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#>> FAISS.from_documents() \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#>> semantic_split_documents의 내용을 OpenAI 임베딩 모델을 통해 고차원 벡터로 변환 \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#>> FAISS 인덱스 생성 \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#>> 위에서 생성된 문서의 벡터를 FAISS 인덱스에 추가 \u001b[39;00m\n",
      "File \u001b[0;32m/root/home/envforir/lib/python3.10/site-packages/langchain_core/vectorstores.py:635\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    634\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/root/home/envforir/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:931\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    930\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[0;32m--> 931\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/root/home/envforir/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:883\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[0;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__from\u001b[39m(\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    882\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[0;32m--> 883\u001b[0m     faiss \u001b[38;5;241m=\u001b[39m \u001b[43mdependable_faiss_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distance_strategy \u001b[38;5;241m==\u001b[39m DistanceStrategy\u001b[38;5;241m.\u001b[39mMAX_INNER_PRODUCT:\n\u001b[1;32m    885\u001b[0m         index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m/root/home/envforir/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:57\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[0;34m(no_avx2)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import faiss python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install faiss-gpu` (for CUDA supported GPU) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `pip install faiss-cpu` (depending on Python version).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m faiss\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version)."
     ]
    }
   ],
   "source": [
    "# semantic_split_documents로 벡터저장소 생성 \n",
    "vectorstore = FAISS.from_documents(documents=semantic_split_documents, embedding=OpenAIEmbeddings())\n",
    "\n",
    "#>> FAISS.from_documents() \n",
    "#>> semantic_split_documents의 내용을 OpenAI 임베딩 모델을 통해 고차원 벡터로 변환 \n",
    "#>> FAISS 인덱스 생성 \n",
    "#>> 위에서 생성된 문서의 벡터를 FAISS 인덱스에 추가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 검색 테스트 \n",
    "query = \"금성에서 달이 어떻게 보일까?\"\n",
    "similar_docs = vectorstore.similarity_search(query, k=3)  # 상위 3개 유사 문서 검색\n",
    "#>> 유클리디안 거리 \n",
    "\n",
    "for doc in similar_docs:\n",
    "    print(f\"내용: {doc.page_content[:100]}...\")\n",
    "    print(f\"메타데이터: {doc.metadata}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 벡터스토어 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envforir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
