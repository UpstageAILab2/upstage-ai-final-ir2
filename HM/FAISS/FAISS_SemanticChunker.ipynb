<<<<<<< HEAD
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pickle\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.query_constructor.base import AttributeInfo, get_query_constructor_prompt\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() \n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 문서 로드(Load Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 4272\n"
     ]
    }
   ],
   "source": [
    "class JSONLLoader(BaseLoader):\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load(self):\n",
    "        documents = []\n",
    "        seq_num = 1\n",
    "        \n",
    "        with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                data = json.loads(line)\n",
    "                doc = Document(\n",
    "                    page_content=data['content'],\n",
    "                    metadata={\n",
    "                        'docid': data['docid'],\n",
    "                        'src': data.get('src', ''),  # 'src' 필드가 없을 경우 빈 문자열 사용\n",
    "                        'source': self.file_path,\n",
    "                        'seq_num': seq_num,\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "                seq_num += 1\n",
    "        \n",
    "        return documents\n",
    "\n",
    "file_path = \"/data/ephemeral/home/upstage-ai-final-ir2/HM/data/documents.jsonl\"\n",
    "loader = JSONLLoader(file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"문서의 수: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 문서 분할(Split Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 4272\n",
      "분할된 문서의 수: 17989\n"
     ]
    }
   ],
   "source": [
    "# RecursiveCharacterTextSplitter \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# 문서 분할\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"문서의 수: {len(documents)}\")\n",
    "print(f\"분할된 문서의 수: {len(split_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 문서의 수: 4272\n",
      "분할된 문서의 수: 4280\n"
     ]
    }
   ],
   "source": [
    "# 문장 단위 TextSplitter\n",
    "class SentenceSplitter(TextSplitter):\n",
    "    def split_text(self, text: str) -> list[str]:\n",
    "        # 문장 끝 패턴: .!?로 끝나고 공백이 따라오는 경우\n",
    "        # 줄바꿈 문자도 문장의 끝으로 간주\n",
    "        return [sentence.strip() for sentence in re.split(r'(?<=[.!?])\\s+|\\n', text) if sentence.strip()]\n",
    "\n",
    "# SentenceSplitter 초기화\n",
    "sentence_splitter = SentenceSplitter()\n",
    "\n",
    "# 문서 분할\n",
    "sentence_split_documents = sentence_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"원본 문서의 수: {len(documents)}\")\n",
    "print(f\"분할된 문서의 수: {len(split_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 문서의 수: 4272\n",
      "분할된 문서의 수: 8540\n"
     ]
    }
   ],
   "source": [
    "# SemanticChunker\n",
    "semantic_text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(), add_start_index=True)\n",
    "\n",
    "# documents를 split\n",
    "semantic_split_documents = semantic_text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"원본 문서의 수: {len(documents)}\")\n",
    "print(f\"분할된 문서의 수: {len(semantic_split_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic_split_documents 저장\n",
    "with open('semantic_split_documents.pkl', 'wb') as f:\n",
    "    pickle.dump(semantic_split_documents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로드된 분할 문서의 수: 8540\n"
     ]
    }
   ],
   "source": [
    "# semantic_split_documents 로드\n",
    "with open('semantic_split_documents.pkl', 'rb') as f:\n",
    "    loaded_split_documents = pickle.load(f)\n",
    "\n",
    "print(f\"로드된 분할 문서의 수: {len(loaded_split_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 임베딩(Embedding) 및 벡터저장소 생성(Create Vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # semantic_split_documents로 벡터저장소 생성 \n",
    "# vectorstore = FAISS.from_documents(documents=semantic_split_documents, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# #>> FAISS.from_documents() \n",
    "# #>> semantic_split_documents의 내용을 OpenAI 임베딩 모델을 통해 고차원 벡터로 변환 \n",
    "# #>> FAISS 인덱스 생성 \n",
    "# #>> 위에서 생성된 문서의 벡터를 FAISS 인덱스에 추가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic_split_documents로 벡터저장소 생성 \n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=OpenAIEmbeddings())\n",
    "\n",
    "#>> FAISS.from_documents() \n",
    "#>> semantic_split_documents의 내용을 OpenAI 임베딩 모델을 통해 고차원 벡터로 변환 \n",
    "#>> FAISS 인덱스 생성 \n",
    "#>> 위에서 생성된 문서의 벡터를 FAISS 인덱스에 추가 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Retriever 생성 및 예제 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 검색 테스트 \n",
    "query = \"나무의 분류에 대해 조사해 보기 위한 방법은?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/home/envforir/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# OpenAI LLM 초기화 \n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/home/envforir/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/root/home/envforir/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: 나무의 분류에 대해 조사해 보기 위한 방법은?\n",
      "Standalone Query: \"나무 분류 조사 방법\"\n"
     ]
    }
   ],
   "source": [
    "# Standalone Query Generator 프롬프트 템플릿\n",
    "standalone_query_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"질문 query를 요약하려고 합니다. 핵심내용을 포함한 주제를 출력해주세요.\n",
    "    아래는 예시입니다. \n",
    "    \n",
    "    원래의 질문: \"금성이 밝게 보이는 이유가 뭐야?\"\n",
    "    생성할 standalone_query: \"금성 밝기 원인\"\n",
    "\n",
    "    원래의 질문: {question}\n",
    "    독립적인 질문:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Standalone Query Generator 체인 생성\n",
    "standalone_query_chain = LLMChain(llm=llm, prompt=standalone_query_prompt)\n",
    "\n",
    "# Standalone Query 생성\n",
    "standalone_query = standalone_query_chain.run(query)\n",
    "\n",
    "print(f\"Original Query: {query}\")\n",
    "print(f\"Standalone Query: {standalone_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "문서 1:\n",
      "내용: 종류별로 유사한 특징을 가지고 있으며, 이는 생물 분류학에서 중요한 기준 중 하나입니다. 따라서 이 학생의 조사 결과는 나무의 분류와 관련된 중요한 정보를 제공할 수 있습니다....\n",
      "메타데이터: {'docid': 'c63b9e3a-716f-423a-9c9b-0bcaa1b9f35d', 'src': 'ko_ai2_arc__ARC_Challenge__test', 'source': '/data/ephemeral/home/upstage-ai-final-ir2/HM/data/documents.jsonl', 'seq_num': 2199}\n",
      "---\n",
      "\n",
      "문서 2:\n",
      "내용: 이러한 특성은 발삼전나무를 식별하는데 도움을 줄 수 있습니다....\n",
      "메타데이터: {'docid': '4f11bc9b-1b9c-47f1-8600-bcdf78db5b92', 'src': 'ko_ai2_arc__ARC_Challenge__test', 'source': '/data/ephemeral/home/upstage-ai-final-ir2/HM/data/documents.jsonl', 'seq_num': 623}\n",
      "---\n",
      "\n",
      "문서 3:\n",
      "내용: 가장 좋은 방법은 날짜와 식물의 높이, 그리고 토양 유형을 함께 기록하는 것입니다. 이렇게 하면 나중에 데이터를 분석하거나 비교할 때 편리하고 정확한 결과를 얻을 수 있습니다....\n",
      "메타데이터: {'docid': 'a153c822-be9f-4346-8558-34365ed7b4f0', 'src': 'ko_ai2_arc__ARC_Challenge__test', 'source': '/data/ephemeral/home/upstage-ai-final-ir2/HM/data/documents.jsonl', 'seq_num': 1414}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# 생성된 Standalone Query를 사용하여 검색 (K*3개 검색)\n",
    "K = 3\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": K*3})\n",
    "search_result = retriever.get_relevant_documents(standalone_query)\n",
    "\n",
    "# 중복 docid 제거 및 상위 3개 선택\n",
    "unique_docs = []\n",
    "seen_docids = set()\n",
    "\n",
    "for doc in search_result:\n",
    "    docid = doc.metadata.get('docid')\n",
    "    if docid not in seen_docids:\n",
    "        unique_docs.append(doc)\n",
    "        seen_docids.add(docid)\n",
    "        if len(unique_docs) == K:\n",
    "            break\n",
    "\n",
    "# 결과 출력\n",
    "for i, doc in enumerate(unique_docs, 1):\n",
    "    print(f\"\\n문서 {i}:\")\n",
    "    print(f\"내용: {doc.page_content[:100]}...\")  # 처음 100자만 출력\n",
    "    print(f\"메타데이터: {doc.metadata}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 제출용 output 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def answer_question(query):\n",
    "#     # Standalone Query 생성\n",
    "#     standalone_query = standalone_query_chain.run(query)\n",
    "\n",
    "#     # 검색 수행 (K*3개 검색)\n",
    "#     K = 3\n",
    "#     retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": K*3})\n",
    "#     search_result = retriever.get_relevant_documents(standalone_query)\n",
    "\n",
    "#     # 중복 docid 제거 및 상위 3개 선택\n",
    "#     unique_docs = []\n",
    "#     seen_docids = set()\n",
    "\n",
    "#     for doc in search_result:\n",
    "#         docid = doc.metadata.get('docid')\n",
    "#         if docid not in seen_docids:\n",
    "#             unique_docs.append(doc)\n",
    "#             seen_docids.add(docid)\n",
    "#             if len(unique_docs) == K:\n",
    "#                 break\n",
    "\n",
    "#     # RAG 프롬프트 가져오기\n",
    "#     rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "#     # 첫 번째 문서의 내용만 사용\n",
    "#     context = unique_docs[0].page_content if unique_docs else \"\"\n",
    "\n",
    "#     # LLM 체인 생성\n",
    "#     llm = OpenAI(temperature=0)\n",
    "#     rag_chain = LLMChain(llm=llm, prompt=rag_prompt)\n",
    "\n",
    "#     # 답변 생성 (첫 번째 문서만 참고)\n",
    "#     answer = rag_chain.run(context=context, question=query)\n",
    "    \n",
    "#     standalone_query = standalone_query_chain.run(query).strip('\"')  # 따옴표 제거\n",
    "\n",
    "#     # topk 및 references 정보 추출\n",
    "#     topk = [doc.metadata.get('docid') for doc in unique_docs]\n",
    "#     references = [\n",
    "#         {\n",
    "#             \"score\": doc.metadata.get('score', 0),\n",
    "#             \"content\": doc.page_content\n",
    "#         }\n",
    "#         for doc in unique_docs\n",
    "#     ]\n",
    "\n",
    "#     return {\n",
    "#         \"standalone_query\": standalone_query,\n",
    "#         \"topk\": topk,\n",
    "#         \"answer\": answer,\n",
    "#         \"references\": references\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_check_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "다음 질문이 과학 상식과 관련된 내용인지 판단해주세요:\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "이 질문이 과학(물리, 화학, 생물, 지구과학, 천문학 등)과 관련된 상식이나 지식을 묻는 것이라면 검색을 진행하여 답변하고, 그렇지 않다면 아무 답변도 하지 마세요.\n",
    "\n",
    "답변:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "science_check_chain = LLMChain(llm=llm, prompt=science_check_prompt)\n",
    "\n",
    "def answer_question(query):\n",
    "    # 과학 상식 관련 질문인지 확인\n",
    "    science_check_result = science_check_chain.run(query).strip().lower()\n",
    "    \n",
    "    if not science_check_result:  # 빈 문자열이면 과학 질문이 아님\n",
    "        return {\n",
    "            \"answer\": \"\",\n",
    "            \"standalone_query\": \"\",\n",
    "            \"topk\": [],\n",
    "            \"references\": []\n",
    "        }\n",
    "\n",
    "    # 여기서부터는 기존 코드와 동일\n",
    "    # Standalone Query 생성\n",
    "    standalone_query = standalone_query_chain.run(query)\n",
    "\n",
    "    # 검색 수행 (K*3개 검색)\n",
    "    K = 3\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": K*3})\n",
    "    search_result = retriever.get_relevant_documents(standalone_query)\n",
    "\n",
    "    # 중복 docid 제거 및 상위 3개 선택\n",
    "    unique_docs = []\n",
    "    seen_docids = set()\n",
    "\n",
    "    for doc in search_result:\n",
    "        docid = doc.metadata.get('docid')\n",
    "        if docid not in seen_docids:\n",
    "            unique_docs.append(doc)\n",
    "            seen_docids.add(docid)\n",
    "            if len(unique_docs) == K:\n",
    "                break\n",
    "\n",
    "    # RAG 프롬프트 가져오기\n",
    "    rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # 첫 번째 문서의 내용만 사용\n",
    "    context = unique_docs[0].page_content if unique_docs else \"\"\n",
    "\n",
    "    # LLM 체인 생성\n",
    "    llm = OpenAI(temperature=0)\n",
    "    rag_chain = LLMChain(llm=llm, prompt=rag_prompt)\n",
    "\n",
    "    # 답변 생성 (첫 번째 문서만 참고)\n",
    "    answer = rag_chain.run(context=context, question=query)\n",
    "    \n",
    "    standalone_query = standalone_query_chain.run(query).strip('\"')  # 따옴표 제거\n",
    "\n",
    "    # topk 및 references 정보 추출\n",
    "    topk = [doc.metadata.get('docid') for doc in unique_docs]\n",
    "    \n",
    "    # 이미 검색된 문서의 내용을 사용\n",
    "    references = [\n",
    "        {\n",
    "            \"score\": doc.metadata.get('score', 0),\n",
    "            \"content\": doc.page_content  # 변경된 부분\n",
    "        }\n",
    "        for doc in unique_docs\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"standalone_query\": standalone_query,\n",
    "        \"topk\": topk,\n",
    "        \"answer\": answer,\n",
    "        \"references\": references\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_rag(eval_filename, output_filename):\n",
    "    with open(eval_filename) as f, open(output_filename, \"w\") as of:\n",
    "        idx = 0\n",
    "        for line in f:\n",
    "            j = json.loads(line)\n",
    "            print(f'Test {idx}\\nQuestion: {j[\"msg\"]}')\n",
    "            response = answer_question(j[\"msg\"])\n",
    "            print(f'Answer: {response[\"answer\"]}\\n')\n",
    "\n",
    "            output = {\n",
    "                \"eval_id\": j[\"eval_id\"],\n",
    "                \"standalone_query\": response[\"standalone_query\"],\n",
    "                \"topk\": response[\"topk\"],\n",
    "                \"answer\": response[\"answer\"],\n",
    "                \"references\": response[\"references\"]\n",
    "            }\n",
    "            of.write(f'{json.dumps(output, ensure_ascii=False)}\\n')\n",
    "            idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OpenAI LLM 초기화 \n",
    "# llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0\n",
      "Question: [{'role': 'user', 'content': '나무의 분류에 대해 조사해 보기 위한 방법은?'}]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FAISS' object has no attribute 'get_document_by_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 평가 실행\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43meval_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/ephemeral/home/upstage-ai-final-ir2/HM/data/eval.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample_submission_7.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m, in \u001b[0;36meval_rag\u001b[0;34m(eval_filename, output_filename)\u001b[0m\n\u001b[1;32m      5\u001b[0m j \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43manswer_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mj\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmsg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m output \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: j[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandalone_query\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandalone_query\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     16\u001b[0m }\n",
      "Cell \u001b[0;32mIn[14], line 68\u001b[0m, in \u001b[0;36manswer_question\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     65\u001b[0m topk \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocid\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m unique_docs]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# 전체 문서 내용을 참조로 사용\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m references \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     69\u001b[0m     {\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: doc\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: vectorstore\u001b[38;5;241m.\u001b[39mget_document_by_id(doc\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocid\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mpage_content\n\u001b[1;32m     72\u001b[0m     }\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m unique_docs\n\u001b[1;32m     74\u001b[0m ]\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandalone_query\u001b[39m\u001b[38;5;124m\"\u001b[39m: standalone_query,\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopk\u001b[39m\u001b[38;5;124m\"\u001b[39m: topk,\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: answer,\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m\"\u001b[39m: references\n\u001b[1;32m     81\u001b[0m }\n",
      "Cell \u001b[0;32mIn[14], line 71\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     65\u001b[0m topk \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocid\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m unique_docs]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# 전체 문서 내용을 참조로 사용\u001b[39;00m\n\u001b[1;32m     68\u001b[0m references \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     69\u001b[0m     {\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: doc\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m---> 71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_document_by_id\u001b[49m(doc\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocid\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mpage_content\n\u001b[1;32m     72\u001b[0m     }\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m unique_docs\n\u001b[1;32m     74\u001b[0m ]\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandalone_query\u001b[39m\u001b[38;5;124m\"\u001b[39m: standalone_query,\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopk\u001b[39m\u001b[38;5;124m\"\u001b[39m: topk,\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: answer,\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m\"\u001b[39m: references\n\u001b[1;32m     81\u001b[0m }\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FAISS' object has no attribute 'get_document_by_id'"
     ]
    }
   ],
   "source": [
    "# 평가 실행\n",
    "eval_rag(\"/data/ephemeral/home/upstage-ai-final-ir2/HM/data/eval.jsonl\", \"sample_submission_7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envforir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
=======
from dotenv import load_dotenv
>>>>>>> 88e006de791add391032331d1e3a345fb160eed3
